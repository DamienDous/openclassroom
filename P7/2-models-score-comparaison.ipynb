{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T09:32:58.327271Z",
     "start_time": "2022-07-12T09:32:58.323598Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T09:33:00.568099Z",
     "start_time": "2022-07-12T09:32:59.133574Z"
    },
    "execution": {
     "iopub.execute_input": "2021-12-08T02:51:26.711178Z",
     "iopub.status.busy": "2021-12-08T02:51:26.710912Z",
     "iopub.status.idle": "2021-12-08T02:51:28.856576Z",
     "shell.execute_reply": "2021-12-08T02:51:28.855584Z",
     "shell.execute_reply.started": "2021-12-08T02:51:26.711147Z"
    }
   },
   "outputs": [],
   "source": [
    "# standard Python tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Main table\n",
    "train = pd.read_pickle('data/global_train_data.pkl').sample(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T09:33:01.433167Z",
     "start_time": "2022-07-12T09:33:01.246342Z"
    },
    "execution": {
     "iopub.execute_input": "2021-12-08T02:51:28.859392Z",
     "iopub.status.busy": "2021-12-08T02:51:28.858410Z",
     "iopub.status.idle": "2021-12-08T02:51:30.958021Z",
     "shell.execute_reply": "2021-12-08T02:51:30.956718Z",
     "shell.execute_reply.started": "2021-12-08T02:51:28.859311Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_size = 0.75\n",
    "\n",
    "y = train['TARGET'].values\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train.drop(\n",
    "    ['TARGET', 'SK_ID_CURR'], axis=1), y, stratify=y, test_size=1 - train_size, random_state=1)\n",
    "print('Shape of X_train:', X_train.shape)\n",
    "print('Shape of y_train:', y_train.shape)\n",
    "print('Shape of X_valid:', X_valid.shape)\n",
    "print('Shape of y_valid:', y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T09:33:05.263156Z",
     "start_time": "2022-07-12T09:33:03.330937Z"
    },
    "execution": {
     "iopub.execute_input": "2021-12-08T02:51:30.975527Z",
     "iopub.status.busy": "2021-12-08T02:51:30.974991Z",
     "iopub.status.idle": "2021-12-08T02:53:27.192383Z",
     "shell.execute_reply": "2021-12-08T02:53:27.191013Z",
     "shell.execute_reply.started": "2021-12-08T02:51:30.975488Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "\n",
    "pipeline.fit(X_train)\n",
    "X_train = pipeline.transform(X_train)\n",
    "X_valid = pipeline.transform(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T09:33:05.973337Z",
     "start_time": "2022-07-12T09:33:05.963394Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Initializing the LogisticRegression\n",
    "LR_clf = LogisticRegression(max_iter=400, solver='lbfgs')\n",
    "LR_parameters = {\"C\": [0.005], \"penalty\": ['l2']}\n",
    "\n",
    "# Initializing the DecisionTreeClassifier\n",
    "DT_clf = DecisionTreeClassifier(random_state=1)\n",
    "DT_parameters = {'max_depth': [8], 'min_samples_leaf': [10]}\n",
    "\n",
    "# Initializing the RandomForestClassifier\n",
    "RF_clf = RandomForestClassifier(random_state=1, n_estimators=100)\n",
    "RF_parameters = {'max_depth': [39],  'min_samples_leaf': [37]}\n",
    "\n",
    "# Initializing the ExtraTreesClassifier\n",
    "XT_clf = ExtraTreesClassifier(random_state=1, class_weight={0: 11.5, 1: 1})\n",
    "XT_params = {'n_estimators': [300], 'min_samples_leaf': [\n",
    "    18], 'min_samples_split': [8], }\n",
    "\n",
    "# Initializing the XGBClassifier\n",
    "XGB_clf = XGBClassifier()\n",
    "XGB_parameters = {'max_depth': [6], 'n_estimators': [\n",
    "    200], 'learning_rate': [0.05], 'gamma': [0]}\n",
    "\n",
    "# Initializing the LGBMClassifier\n",
    "params = {'boosting_type': 'gbdt', 'objective': 'binary', 'max_depth': 18,\n",
    "          'n_jobs': -1, 'num_leaves': 30, 'n_estimators': 1600,\n",
    "          'max_bin': 512, 'subsample_for_bin': 200, 'subsample': 0.8,\n",
    "          'subsample_freq': 1, 'colsample_bytree': 0.8,\n",
    "          'reg_alpha': 80, 'reg_lambda': 20,\n",
    "          'min_split_gain': 0.5, 'min_child_weight': 1,\n",
    "          'min_child_samples': 10, 'scale_pos_weight': 11.5, 'num_class': 1,\n",
    "          'metric': 'roc_auc'\n",
    "          }\n",
    "LGB_clf = LGBMClassifier(**params)\n",
    "LGB_parameters = {'learning_rate': [0.02]}\n",
    "\n",
    "# Initializing the MLPClassifier\n",
    "params = {'activation': 'relu', 'solver': 'adam', 'random_state': 1,\n",
    "          'learning_rate': 'constant', 'max_iter': 1000, 'alpha': 0.0001,\n",
    "          'learning_rate_init': 0.0001\n",
    "          }\n",
    "MLP_clf = MLPClassifier(**params)\n",
    "MLP_parameters = {'hidden_layer_sizes': [(400, 50, 4)]}\n",
    "\n",
    "# Initializing the AdaBoostClassifier\n",
    "AB_clf = AdaBoostClassifier(random_state = 7)\n",
    "AB_parameters = {'n_estimators': [100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T09:33:06.634535Z",
     "start_time": "2022-07-12T09:33:06.631230Z"
    }
   },
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'LogisticRegression': [LR_clf, LR_parameters],\n",
    "    'DecisionTreeClassifier': [DT_clf, DT_parameters],\n",
    "    'RandomForestClassifier': [RF_clf, RF_parameters],\n",
    "    'ExtraTreesClassifier': [XT_clf, XT_params],\n",
    "    'XGBClassifier': [XGB_clf, XGB_parameters],\n",
    "    'LGBMClassifier': [LGB_clf, LGB_parameters],\n",
    "    'MLPClassifier': [MLP_clf, MLP_parameters],\n",
    "    'AdaBoostClassifier': [AB_clf, AB_parameters],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit classifiers on data and get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T09:33:07.336860Z",
     "start_time": "2022-07-12T09:33:07.322829Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def get_statistics(_y_valid, _y_pred):\n",
    "    TN, FP, FN, TP = confusion_matrix(\n",
    "        list(_y_valid), list(_y_pred), labels=[0, 1]).ravel()\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    sensitivity = TP/(TP+FN) if TP+FN != 0 else 0\n",
    "    # Specificity or true negative rate\n",
    "    specifity = TN/(TN+FP) if TN+FP != 0 else 0\n",
    "    # Precision or positive predictive value\n",
    "    precision = TP/(TP+FP) if TP+FP != 0 else 0\n",
    "    # Overall accuracy\n",
    "    accuracy = (TP+TN)/(TP+FP+FN+TN) if TP+FP+FN+TN != 0 else 0\n",
    "\n",
    "    return TN, FP, FN, TP, sensitivity, specifity, precision, accuracy\n",
    "\n",
    "\n",
    "def eval_error(_y_valid, _y_pred):\n",
    "    TN, FP, FN, TP = confusion_matrix(\n",
    "        list(_y_valid), list(_y_pred), labels=[0, 1]).ravel()\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    sensitivity = TP/(TP+FN) if TP+FN != 0 else 0\n",
    "    # Overall accuracy\n",
    "    accuracy = (TP+TN)/(TP+FP+FN+TN) if TP+FP+FN+TN != 0 else 0\n",
    "    error = sensitivity*accuracy\n",
    "    return error\n",
    "\n",
    "\n",
    "def get_cross_validation_fitting_result(classifier_name, classifier, parameters):\n",
    "    \n",
    "    my_scorer = make_scorer(eval_error, greater_is_better=True)\n",
    "\n",
    "    clf_grid = GridSearchCV(classifier, parameters, cv=10,\n",
    "                            refit='True', n_jobs=-1, verbose=1, scoring=my_scorer)\n",
    "    clf_grid.fit(X_train, y_train)\n",
    "    clf_model = clf_grid.best_estimator_\n",
    "    y_pred_proba = clf_model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = clf_model.predict(X_valid)\n",
    "\n",
    "    TN, FP, FN, TP, sensitivity, specifity, precision, accuracy = get_statistics(\n",
    "        y_valid, y_pred)\n",
    "\n",
    "    result = pd.DataFrame({'Model Type': classifier_name,\n",
    "                         'TN': [TN], 'FP': [FP], 'FN': [FN], 'TP': [TP],\n",
    "                         'sensitivity': [sensitivity], 'specifity': [specifity],\n",
    "                         'precision': [precision], 'accuracy': [accuracy],\n",
    "                         'error': [eval_error(y_valid, y_pred)], 'AUC - 10xv': [clf_grid.best_score_],\n",
    "                         'AUC - Valid': [roc_auc_score(y_valid, y_pred_proba, average=None)],\n",
    "                         'Hyperparameters': [clf_grid.best_params_]})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T09:56:42.979881Z",
     "start_time": "2022-07-12T09:33:08.078107Z"
    }
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = ['Model Type', 'TN', 'FP', 'FN', 'TP',\n",
    "                                  'sensitivity', 'specifity', 'precision', 'accuracy',\n",
    "                                  'error', 'AUC - 10xv', 'AUC - Valid', 'Hyperparameters'])\n",
    "\n",
    "for item in classifiers.items():\n",
    "    with timer(item[0]):\n",
    "        result = get_cross_validation_fitting_result(item[0], item[1][0], item[1][1])\n",
    "    \n",
    "        # update model scoreboard\n",
    "        results = pd.concat([results, result], ignore_index=True)\n",
    "    \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T09:31:50.156417Z",
     "start_time": "2022-07-12T09:31:50.129377Z"
    }
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "630px",
    "left": "1107px",
    "right": "20px",
    "top": "110px",
    "width": "556px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
