{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e1736bb",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c97e095",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T10:10:26.016897Z",
     "start_time": "2022-07-26T10:10:17.042360Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import re\n",
    "\n",
    "\n",
    "data = pd.read_pickle('data/global_train_data.pkl').sample(10000)\n",
    "data = data.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "data = data.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Get X and y from data\n",
    "y = data['TARGET'].values\n",
    "X = data.drop(['TARGET', 'SK_ID_CURR'], axis=1)\n",
    "\n",
    "# preprocessing steps\n",
    "pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "X_prepro = pd.DataFrame(pipe.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Split in train and test\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_prepro, y, stratify=y, random_state=1)\n",
    "\n",
    "print('Shape of X_train:', X_train.shape)\n",
    "print('Shape of X_valid:', X_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7b3dea",
   "metadata": {},
   "source": [
    "# Models fitting on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ebe7b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T10:10:31.867028Z",
     "start_time": "2022-07-26T10:10:26.554060Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "params = {'boosting_type': 'gbdt', 'objective': 'binary', 'max_depth': 18,\n",
    "          'n_jobs': -1, 'num_leaves': 30, 'learning_rate': 0.02, 'n_estimators': 1600,\n",
    "          'max_bin': 512, 'subsample_for_bin': 200, 'subsample': 0.8,\n",
    "          'subsample_freq': 1, 'colsample_bytree': 0.8,\n",
    "          'reg_alpha': 80, 'reg_lambda': 20,\n",
    "          'min_split_gain': 0.5, 'min_child_weight': 1,\n",
    "          'min_child_samples': 10, 'scale_pos_weight': 11.5, 'num_class': 1,\n",
    "          'metric': 'auc', 'learning_rate': 0.02\n",
    "          }\n",
    "\n",
    "LGB_clf = lgb.LGBMClassifier(**params)\n",
    "LGB_clf.fit(X_train, y_train)\n",
    "\n",
    "# evaluate predictions\n",
    "print('AUC: ', roc_auc_score(\n",
    "    y_valid, LGB_clf.predict_proba(X_valid)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e010dc69",
   "metadata": {},
   "source": [
    "# Explaining models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2d6264",
   "metadata": {},
   "source": [
    "## LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8811b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T10:35:46.676545Z",
     "start_time": "2022-07-15T10:35:43.610450Z"
    }
   },
   "outputs": [],
   "source": [
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "def get_lime_explainer(model, data, labels):  \n",
    "\n",
    "    cat_feat_ix = [i for i,c in enumerate(data.columns) if pd.api.types.is_categorical_dtype(data[c])]\n",
    "    feat_names = list(data.columns)\n",
    "    class_names = list(set(labels))\n",
    "    lime_explainer = LimeTabularExplainer(data,\n",
    "                                      feature_names=feat_names,\n",
    "                                      class_names=class_names,\n",
    "                                      categorical_features=cat_feat_ix ,\n",
    "                                      mode=\"classification\"\n",
    "                                      )\n",
    "    return lime_explainer\n",
    "\n",
    "def lime_explain(explainer, data, predict_method, num_features): \n",
    "    explanation = explainer.explain_instance(data, predict_method, num_features=num_features) \n",
    "    return explanation\n",
    "\n",
    "lime_data_explainations = []\n",
    "lime_metrics = []\n",
    "lime_explanation_time = []\n",
    "feat_names = list(X.columns)\n",
    "\n",
    "\n",
    "predict_method = LGB_clf.predict_proba \n",
    "\n",
    "# explain first sample from test data\n",
    "lime_explainer = get_lime_explainer(LGB_clf, X_train, y_train)\n",
    "explanation = lime_explain(lime_explainer, scaled_test_data[6], predict_method, top_x) \n",
    "\n",
    "ex_holder = {}\n",
    "for feat_index,ex in explanation.as_map()[1] :\n",
    "    ex_holder[feat_names[feat_index]] = ex\n",
    "\n",
    "lime_data_explainations.append(ex_holder) \n",
    "actual_pred = predict_method(scaled_test_data[6].reshape(1,-1))\n",
    "perc_pred_diff =  abs(actual_pred[0][1] - explanation.local_pred[0])   \n",
    "lime_explanation_time.append({\"time\": elapsed_time, \"model\": \"LGB\" })\n",
    "lime_metrics.append({\"lime class1\": explanation.local_pred[0], \"actual class1\": actual_pred[0][1], \"class_diff\": round(perc_pred_diff,3), \"model\": current_model[\"name\"] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d650d34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T10:36:09.385418Z",
     "start_time": "2022-07-15T10:36:09.242206Z"
    }
   },
   "outputs": [],
   "source": [
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "\n",
    "def get_lime_explainer(model, data, labels):\n",
    "\n",
    "    feat_names = list(data.columns)\n",
    "    class_names = list(set(labels))\n",
    "    lime_explainer = LimeTabularExplainer(data,\n",
    "                                          feature_names=feat_names,\n",
    "                                          class_names=class_names,\n",
    "                                          mode=\"classification\"\n",
    "                                          )\n",
    "    return lime_explainer\n",
    "\n",
    "\n",
    "def lime_explain(explainer, data, predict_method, num_features):\n",
    "    explanation = explainer.explain_instance(\n",
    "        data, predict_method, num_features=num_features)\n",
    "    return explanation\n",
    "\n",
    "\n",
    "# explain first sample from test data\n",
    "lime_explainer = get_lime_explainer(LGB_clf, X_train, y_train)\n",
    "explanation = lime_explain(lime_explainer, X_test,\n",
    "                           LGB_clf.predict_proba, top_x)\n",
    "\n",
    "ex_holder = {}\n",
    "for feat_index, ex in explanation.as_map()[1]:\n",
    "    ex_holder[X.columns[feat_index]] = ex\n",
    "\n",
    "actual_pred = LGB_clf.predict_proba(X_test.reshape(1, -1))\n",
    "perc_pred_diff = abs(actual_pred[0][1] - explanation.local_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015defd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-29T14:46:05.540472Z",
     "start_time": "2022-06-29T14:46:04.408765Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_lime_exp(fig, fig_index, exp_data, title):\n",
    "    features =  list(exp_data.keys())[::-1]\n",
    "    features = [string[:10] for string in features]\n",
    "    explanations = list(exp_data.values())[::-1]\n",
    "    ax = fig.add_subplot(fig_index) \n",
    "    lime_bar = ax.barh( features, explanations ) \n",
    "    ax.set_title(title, fontsize = 20)\n",
    "    for i,bar in enumerate(lime_bar):\n",
    "        cols_abrev = [string[:10] for string in X.columns]\n",
    "        bar.set_color(color_list[list(cols_abrev).index(features[i])])\n",
    "        plt.box(False) \n",
    "fig = plt.figure(figsize=(19,8))\n",
    "\n",
    "# Plot lime explanations for trained models\n",
    "for i, dex in enumerate(lime_data_explainations):\n",
    "    fig_index = \"23\" + str(i+1)\n",
    "    plot_lime_exp(fig, int(fig_index), lime_data_explainations[i], trained_models[i][\"name\"])\n",
    "\n",
    "plt.suptitle( \" LIME Explanation for single test data instance.  Top \" + str(top_x) + \" Features\", fontsize=20, fontweight=\"normal\")\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "# Plot run time for explanations\n",
    "lx_df = pd.DataFrame(lime_explanation_time)\n",
    "lx_df.sort_values(\"time\", inplace=True)\n",
    "setup_plot()\n",
    "lx_ax = lx_df.plot(kind=\"line\", x=\"model\", title=\"Runtime (seconds) for single test data instance LIME explanation\", figsize=(22,6))\n",
    "lx_ax.title.set_size(20)\n",
    "lx_ax.legend([\"Run time\"])\n",
    "plt.box(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30005ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-29T14:46:11.725548Z",
     "start_time": "2022-06-29T14:46:11.456977Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot run time for explanations\n",
    "lime_metrics_df = pd.DataFrame(lime_metrics)  \n",
    "lime_metrics_df_ax = lime_metrics_df[[\"lime class1\", \"actual class1\", \"model\"]].plot(kind=\"line\", x=\"model\", title=\"LIME Actual Prediction vs Local Prediction \", figsize=(22,6))\n",
    "lime_metrics_df_ax.title.set_size(20)\n",
    "lime_metrics_df_ax.legend([\"Lime Local Prediction\", \"Actual Prediction\"])\n",
    "plt.box(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2963634f",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceec20f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T10:10:34.258676Z",
     "start_time": "2022-07-26T10:10:33.600555Z"
    }
   },
   "outputs": [],
   "source": [
    "current_model = trained_models[3] # Explain the Random Forest Model\n",
    "clf = current_model[\"model\"][\"clf\"]\n",
    "scaler = current_model[\"model\"][\"scaler\"]\n",
    "scaled_train_data = scaler.transform(X_train)\n",
    "sub_sampled_train_data = shap.sample(scaled_train_data, 600, random_state=0) # use 600 samples of train data as background data\n",
    "\n",
    "scaled_test_data = scaler.transform(X_test) \n",
    "subsampled_test_data =scaled_test_data[test_data_index].reshape(1,-1)\n",
    "\n",
    "start_time = time.time()\n",
    "explainer = shap.KernelExplainer(clf.predict_proba, sub_sampled_train_data)\n",
    "shap_values = explainer.shap_values(subsampled_test_data,  l1_reg=\"aic\")\n",
    "elapsed_time = time.time() - start_time\n",
    "# explain first sample from test data\n",
    "print(\"Kernel Explainer SHAP run time\", round(elapsed_time,3) , \" seconds. \", current_model[\"name\"])\n",
    "print(\"SHAP expected value\", explainer.expected_value)\n",
    "print(\"Model mean value\", clf.predict_proba(scaled_train_data).mean(axis=0))\n",
    "print(\"Model prediction for test data\", clf.predict_proba(subsampled_test_data))\n",
    "shap.initjs()\n",
    "pred_ind = 0\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1][0], subsampled_test_data[0], feature_names=X_train.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6140cfda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-29T14:47:35.568841Z",
     "start_time": "2022-06-29T14:47:35.259711Z"
    }
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.summary_plot(shap_values, subsampled_test_data, feature_names=X_train.columns, max_display=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785141b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-29T14:55:17.918986Z",
     "start_time": "2022-06-29T14:55:17.441432Z"
    }
   },
   "outputs": [],
   "source": [
    "current_model = trained_models[3]\n",
    "clf = current_model[\"model\"][\"clf\"]\n",
    "scaler = current_model[\"model\"][\"scaler\"]\n",
    "\n",
    "scaled_test_data = scaler.transform(X_test) \n",
    "subsampled_test_data =scaled_test_data[test_data_index].reshape(1,-1)\n",
    "\n",
    "# explain first sample from test data\n",
    "start_time = time.time()\n",
    "explainer = shap.TreeExplainer(clf)\n",
    "shap_values = explainer.shap_values(subsampled_test_data)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\"Tree Explainer SHAP run time\", round(elapsed_time,3) , \" seconds. \", current_model[\"name\"])\n",
    "print(\"SHAP expected value\", explainer.expected_value)\n",
    "print(\"Model mean value\", clf.predict_proba(scaled_train_data).mean(axis=0))\n",
    "print(\"Model prediction for test data\", clf.predict_proba(subsampled_test_data))\n",
    "shap.initjs()\n",
    "pred_ind = 0\n",
    "shap.force_plot(explainer.expected_value, shap_values[0], subsampled_test_data[0], feature_names=X_train.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b45813",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-29T14:58:04.543198Z",
     "start_time": "2022-06-29T14:55:41.785556Z"
    }
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "def get_kernel_shap_explainer(model, background_data, train_data):  \n",
    "    shap_explainer = shap.KernelExplainer(model.predict_proba, background_data)   \n",
    "    return shap_explainer \n",
    "\n",
    "def shap_explain(explainer, test_data): \n",
    "    shap_values = explainer.shap_values(test_data, l1_reg=\"aic\")\n",
    "\n",
    "    return shap_values\n",
    "\n",
    "shap_data_explainations = []\n",
    "shape_explanation_time = []\n",
    "feat_names = list(X.columns) \n",
    "data_subsample = 500 \n",
    "for current_model in trained_models:  \n",
    "    scaler = current_model[\"model\"][\"scaler\"]\n",
    "    scaled_test_data = scaler.transform(X_test)\n",
    "    scaled_train_data = scaler.transform(X_train)\n",
    "    sampled_scaled_train_data = shap.sample(scaled_train_data, data_subsample) # subsample background data to make things faster\n",
    "\n",
    "    start_time = time.time()\n",
    "    shap_explainer  = get_kernel_shap_explainer(current_model[\"model\"][\"clf\"], sampled_scaled_train_data, scaled_train_data)\n",
    "\n",
    "    # explain first sample from test data \n",
    "    sampled_scaled_test_data = scaled_test_data[test_data_index].reshape(1,-1)\n",
    "    shap_values = shap_explain(shap_explainer, sampled_scaled_test_data) \n",
    "    elapsed_time = time.time() - start_time \n",
    "    idx = np.argsort(np.abs(shap_values[1][0]))[::-1] \n",
    "    ex_holder = { feat_names[idx[i]] : shap_values[1][0][idx[i]] for i in range(top_x)} \n",
    "\n",
    "\n",
    "    shap_data_explainations.append(ex_holder) \n",
    "    shape_explanation_time.append({\"time\": elapsed_time, \"model\": current_model[\"name\"] })\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b4ceef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-29T15:31:30.155207Z",
     "start_time": "2022-06-29T15:31:28.651991Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_shap_exp(fig, fig_index, exp_data, title):\n",
    "    features =  list(exp_data.keys())[::-1]\n",
    "    explanations = list(exp_data.values())[::-1]\n",
    "    ax = fig.add_subplot(fig_index) \n",
    "    lime_bar = ax.barh( features, explanations ) \n",
    "    ax.set_title(title, fontsize = 20)\n",
    "    for i,bar in enumerate(lime_bar):\n",
    "        bar.set_color(color_list[list(current_data.columns).index(features[i])])\n",
    "        plt.box(False) \n",
    "\n",
    "\n",
    "# Plot SHAP explanations for a given test set item\n",
    "fig = plt.figure(figsize=(19,8))\n",
    "for i, dex in enumerate(shap_data_explainations):\n",
    "    fig_index = \"23\" + str(i+1)\n",
    "    plot_lime_exp(fig, int(fig_index), shap_data_explainations[i], trained_models[i][\"name\"])\n",
    "\n",
    "plt.suptitle( \"Kernel SHAP Explanation for single test data instance.  Top \" + str(top_x) + \" Features\", fontsize=20, fontweight=\"normal\")\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "# Plot SHAP explanation run time\n",
    "shapx_df = pd.DataFrame(shape_explanation_time)\n",
    "shapx_df.sort_values(\"time\", inplace=True)\n",
    "# setup_plot()\n",
    "# shapx_df_ax = shapx_df.plot(kind=\"line\", x=\"model\", title=\"Runtime (seconds) for single instance SHAP explanation\", figsize=(22,6))\n",
    "# shapx_df_ax.title.set_size(20)\n",
    "# shapx_df_ax.legend([\"Run time\"])\n",
    "# plt.box(False)\n",
    "\n",
    "\n",
    "# Plot both LIME and SHAP explanation run times\n",
    "m_df =  shapx_df.merge(lx_df, on=\"model\", suffixes=(\"_SHAP\", \"_LIME\")) \n",
    "m_df.head() \n",
    "mx_df_ax = m_df.plot(kind=\"line\", x=\"model\", title=\"Kernel SHAP vs LIME: Runtime (seconds) for single instance explanation\", figsize=(22,6))\n",
    "mx_df_ax.title.set_size(20)\n",
    "mx_df_ax.legend([\"Run time for SHAP\", \"Run time for LIME\"])\n",
    "plt.box(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3bfb3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-29T15:28:00.032837Z",
     "start_time": "2022-06-29T14:58:49.241635Z"
    }
   },
   "outputs": [],
   "source": [
    "current_model = trained_models[3]\n",
    "clf = current_model[\"model\"][\"clf\"]\n",
    "scaler = current_model[\"model\"][\"scaler\"]\n",
    "\n",
    "sample_sizes = [50, 100, 150, 200, 300, 600, 1000, 2000]\n",
    "metric_holder = []\n",
    " \n",
    "\n",
    "for sample_size in sample_sizes:\n",
    "    scaled_train_data = scaler.transform(X_train)\n",
    "    sub_sampled_train_data = shap.sample(scaled_train_data, sample_size, random_state=0) # use x samples of train data as background data\n",
    "\n",
    "    scaled_test_data = scaler.transform(X_test)\n",
    "    test_data_index = 10\n",
    "    subsampled_test_data =scaled_test_data[test_data_index].reshape(1,-1)\n",
    "\n",
    "    start_time = time.time()\n",
    "    explainer = shap.KernelExplainer(clf.predict_proba, sub_sampled_train_data)\n",
    "    shap_values = explainer.shap_values(subsampled_test_data,  l1_reg=\"aic\")\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    metric_holder.append({\"class0 exp\": explainer.expected_value[0], \n",
    "                        \"class1 exp\":explainer.expected_value[1],\n",
    "                        \"run time\": elapsed_time,\n",
    "                        \"sample size\": sample_size\n",
    "                        })\n",
    "    \n",
    "metric_df = pd.DataFrame(metric_holder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d957a4c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-29T15:30:47.086639Z",
     "start_time": "2022-06-29T15:30:46.208417Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_pred = clf.predict_proba(scaled_train_data).mean(axis=0)\n",
    "metric_df[\"mean prediction class0\"] = mean_pred[0]\n",
    "metric_df[\"mean prediction class1\"] = mean_pred[1]\n",
    "metric_df.head()\n",
    "setup_plot()\n",
    "metrix_ax = metric_df[[\"sample size\", \"run time\"]].plot(\n",
    "    kind=\"line\", x=\"sample size\", title=\"Kernel SHAP Run time (seconds) vs Background Data Size\", figsize=(22, 4))\n",
    "metrix_ax.title.set_size(20)\n",
    "plt.box(False)\n",
    "\n",
    "setup_plot()\n",
    "metrix_ax = metric_df[[\"sample size\", \"mean prediction class0\", \"class0 exp\"]].plot(\n",
    "    kind=\"line\", x=\"sample size\", title=\"Expected Value vs Mean Prediction\", figsize=(22, 4))\n",
    "metrix_ax.title.set_size(20)\n",
    "metrix_ax.legend([\"Mean prediction on train set\", \"SHAP Expected Value\"])\n",
    "plt.box(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d3dba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
